{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import variational_bayes as vb\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "import scipy.stats\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.dpi'] = 144\n",
    "rcParams['scatter.marker'] = '.'\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterableAccessor:\n",
    "    def __init__(self, iterable):\n",
    "        self._iterable = iterable\n",
    "        \n",
    "    def __getitem__(self, y):\n",
    "        return [item[y] for item in self._iterable]\n",
    "    \n",
    "accessor = IterableAccessor([{'a': 1}, {'a': 2}])\n",
    "accessor['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sizes = [20, 50, 100]\n",
    "for size in list_sizes:\n",
    "    list_fraction = np.linspace(0.9, 1.1, 21)\n",
    "    list_num_samples = list_fraction * size\n",
    "    num_runs = 100\n",
    "    pd = []\n",
    "    for num_samples in list_num_samples:\n",
    "        num_samples = int(num_samples)\n",
    "        pd.append([vb.is_positive_definite(np.cov(np.random.normal(0, 1, (num_samples, size)), rowvar=False))\n",
    "                   for _ in range(num_runs)])\n",
    "\n",
    "    plt.errorbar(list_fraction, np.mean(pd, axis=1), vb.std_mean(pd, axis=1),\n",
    "                 marker='.', label=str(size))\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_groups = 3\n",
    "num_nodes = 50\n",
    "order = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = 0.025 * 2 ** np.arange(11)\n",
    "list_num_steps = (num_nodes * (num_nodes * order + 1) * fractions).astype(int)\n",
    "list_num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_parameters():\n",
    "    # Generate group sizes and groups\n",
    "    density = np.random.dirichlet(100 * np.ones(num_groups)) # was 10 when things didn't work\n",
    "    z = np.random.choice(num_groups, num_nodes, p=density)\n",
    "    onehot = np.zeros((num_nodes, num_groups))\n",
    "    onehot[np.arange(num_nodes), z] = 1\n",
    "\n",
    "    # Sample noise precisions for all groups\n",
    "    noise_precision = np.random.gamma(5000, size=num_groups)\n",
    "    # noise_precision = 100\n",
    "\n",
    "    # Sample means and precisions of autoregressive coefficients\n",
    "    adjacency_mean = np.random.normal(0, 1e-2, size=(num_groups, num_groups, order))\n",
    "    adjacency_precision = scipy.stats.wishart.rvs(1e5, np.eye(order), size=(num_groups, num_groups))\n",
    "    if adjacency_precision.ndim < 4:\n",
    "        adjacency_precision = adjacency_precision.reshape((num_groups, num_groups, 1, 1))\n",
    "\n",
    "    # Sample the means and precisions of the bias\n",
    "    bias_mean = np.random.normal(0, 0.1, num_groups)\n",
    "    bias_precision = np.random.gamma(1e4, 1, num_groups)\n",
    "\n",
    "    # Sample the matrix of autoregressive coefficients\n",
    "    cholesky = np.linalg.cholesky(np.linalg.inv(adjacency_precision))\n",
    "    cholesky = cholesky[z[:, None], z[None, :]]\n",
    "    adjacency = adjacency_mean[z[:, None], z[None, :]] + \\\n",
    "        np.einsum('...ij,...j', cholesky, np.random.normal(0, 1, (num_nodes, num_nodes, order)))\n",
    "\n",
    "    # Sample the bias\n",
    "    bias = np.random.normal(0, 1, num_nodes) / np.sqrt(bias_precision[z]) + bias_mean[z]\n",
    "\n",
    "    # Construct the coefficients for comparison\n",
    "    coefficients = vb.pack_coefficients(adjacency, bias)\n",
    "    \n",
    "    return {\n",
    "        'coefficients': coefficients,\n",
    "        'bias': bias,\n",
    "        'adjacency': adjacency,\n",
    "        'z': z,\n",
    "        'noise_precision': noise_precision,\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!rm var-sample-size.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(i, tqdm=False):\n",
    "    np.random.seed(3 + i)\n",
    "    result = defaultdict(list)\n",
    "    parameters = simulate_parameters()\n",
    "    result['parameters'] = parameters\n",
    "    \n",
    "    for num_steps in tqdm_notebook(list_num_steps) if tqdm else list_num_steps:\n",
    "        series = vb.simulate_series(parameters['bias'], parameters['adjacency'], \n",
    "                                    parameters['noise_precision'][parameters['z']], int(num_steps))\n",
    "\n",
    "        # Model without hierarchical structure\n",
    "        model_naive = vb.naive_var_model(series, order)\n",
    "        model_naive.update(None, convergence_predicate=1e-3)\n",
    "        \n",
    "        # Model with hierarchical structure\n",
    "        ensemble_full = vb.ModelEnsemble(vb.var_model, (series, order, num_groups), {'shared_noise': False})\n",
    "        ensemble_full.update(10, None, convergence_predicate=1e-3)\n",
    "        model_full = ensemble_full.best_model\n",
    "        \n",
    "        # Partial model with the inferred coefficients given\n",
    "        ensemble_partial = vb.ModelEnsemble(vb.var_model, (series, order, num_groups), \n",
    "                                            {\n",
    "                                                'shared_noise': False, \n",
    "                                                 'given': {'coefficients': model_naive['coefficients'].mean}\n",
    "                                            })\n",
    "        ensemble_partial.update(10, None, convergence_predicate=1e-3)\n",
    "        model_partial = ensemble_partial.best_model\n",
    "        \n",
    "        result['means_naive'].append(model_naive['coefficients'].mean)\n",
    "        result['stds_naive'].append(model_naive['coefficients'].std)\n",
    "        \n",
    "        result['means_full'].append(model_full['coefficients'].mean)\n",
    "        result['stds_full'].append(model_full['coefficients'].std)\n",
    "        result['zs_full'].append(model_full['z'].mean)\n",
    "        \n",
    "        result['zs_partial'].append(model_partial['z'].mean)\n",
    "        \n",
    "        \n",
    "    result = {key: np.asarray(value) if isinstance(value, list) else value for key, value in result.items()}\n",
    "    return result\n",
    "\n",
    "filename = 'var-sample-size.pickle'\n",
    "if os.path.isfile(filename):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        results = pickle.load(fp)\n",
    "else:\n",
    "    num_runs = 4\n",
    "    results = []\n",
    "    with multiprocessing.Pool(4) as pool:\n",
    "        for result in tqdm_notebook(pool.imap_unordered(main, range(num_runs))):\n",
    "            results.append(result)\n",
    "    \"\"\"    \n",
    "    for _ in tqdm_notebook(range(num_runs)):\n",
    "        results.append(main(tqdm=True))\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(results, fp)\n",
    "\n",
    "results = IterableAccessor(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [(ax1, ax2), (ax3, ax4)] = plt.subplots(2, 2, True)\n",
    "sigmas = 2\n",
    "\n",
    "for list_scores, label in zip([np.divide(results['means_naive'], results['stds_naive']),\n",
    "                               np.divide(results['means_full'], results['stds_full'])],\n",
    "                              ['VAR', 'HVAR']):\n",
    "    significiant = np.mean(np.abs(list_scores) > sigmas, (2, 3))\n",
    "    ax1.errorbar(fractions, np.mean(significiant, axis=0), np.std(significiant, axis=0) / np.sqrt(num_runs - 1),\n",
    "                 label=label)\n",
    "    \n",
    "list_coefficients = np.asarray(IterableAccessor(results['parameters'])['coefficients'])[:, None]\n",
    "for residuals in [np.subtract(results['means_naive'], list_coefficients),\n",
    "                  np.subtract(results['means_full'], list_coefficients)]:\n",
    "    rmse = np.sqrt(np.mean(residuals * residuals, axis=(2, 3)))\n",
    "    ax4.errorbar(fractions, np.mean(rmse, axis=0), np.std(rmse, axis=0) / np.sqrt(num_runs - 1))\n",
    "    \n",
    "    \n",
    "for i, list_zs in enumerate([results['zs_full'], results['zs_partial']]):\n",
    "    list_nmis = []\n",
    "    for parameters, zs in zip(results['parameters'], list_zs):\n",
    "        nmis = [metrics.normalized_mutual_info_score(parameters['z'], np.argmax(z, axis=1)) for z in zs]\n",
    "        list_nmis.append(nmis)\n",
    "\n",
    "    ax3.errorbar(fractions, np.mean(list_nmis, axis=0), \n",
    "                 np.std(list_nmis, axis=0) / np.sqrt(num_runs - 1), color='C%d' % (i + 1))\n",
    "\n",
    "for ax in [ax1, ax3, ax4]:\n",
    "    ax.axvline(1, ls=':', color='k')\n",
    "    \n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylabel(r'$\\mathrm{avg}\\left[\\left|\\frac{\\hat\\theta}{\\sigma}\\right| > 2\\right]$')\n",
    "\n",
    "ax2.set_axis_off()\n",
    "ax2.text(0, 1, \"$n=%d$\\n$p=%d$\\n$K=%d$\" % (num_nodes, order, num_groups), \n",
    "         transform=ax2.transAxes, va='top')\n",
    "ax2.legend()\n",
    "\n",
    "ax3.set_xlabel('Fraction $f$')\n",
    "ax3.set_ylabel('NMI')\n",
    "\n",
    "ax4.set_xlabel('Fraction $f$')\n",
    "ax4.set_ylabel('RMSE')\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['means_naive'][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
